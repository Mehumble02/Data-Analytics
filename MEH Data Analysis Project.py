# -*- coding: utf-8 -*-
"""Data Analysis Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0zZ-LUGEs6ekgC8lolyjyfBKUjiKnEU
"""

# Import necessary libraries
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer     # to convert into a numeric matrix

# Get data
df = pd.read_csv("healthcare_dataset.csv")

print(df)    # to check the data is loaded

# Import additional required libraries
import seaborn as sns
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display, clear_output

# Create a new column for age buckets (20-year intervals) to increase readability of the graphs
# "labels = " code section generated by chatGPT. I could not get the labels correctly formatted
df['Age Group'] = pd.cut(df['Age'], bins=range(0, 120, 20), right=False, labels=[f'{i}-{i+19}' for i in range(0, 100, 20)])

# Add in a dropdown filter for the user to interact with
dropdown = widgets.Dropdown(
    options = ['Age Group', 'Gender', 'Blood Type'],  # limiting the options to be user friendly and keep only relevant data
    value = 'Gender',  # Default value
    description = 'Select:',
    disabled = False,
)

# Implement the user's choice
def plot_MC(hue_variable):
    plt.figure(figsize = (15, 10))   # Larger plot size due to the size of the blood type graph
    ax = sns.countplot(x = 'Medical Condition', hue = hue_variable, data = df, palette = 'rainbow')  # Choosing rainbow color palette for both aesthetic and readability
    plt.title(f'Medical Conditions by {hue_variable}')   # Add title
    plt.xlabel('Medical Condition')    # X-axis label
    plt.ylabel('Patient Count')    # Y-axis label
    plt.legend(title = hue_variable)   # Add in the legend
    plt.xticks(rotation = 45)

    # Display the actual count values (solution created in collaboration with chatGPT)
    for p in ax.patches: # iterate through each bar
        ax.annotate(format(p.get_height(), '.0f'),  # add annotations, get height into string to find value
                    (p.get_x() + p.get_width() / 2., p.get_height()), # States where the text will be displayed (gotten by chatGPT)
                    ha = 'center', va = 'center', # Center the text
                    xytext = (0, 10),
                    textcoords = 'offset points') # States that the text will be offset (not too sure. But it works)

    plt.show()    # Display the graph

# Created to allow the user to change the graph based on the filter (this did not work unless it was at the bottom)
def on_dropdown_change(change):
    clear_output(wait = True)  # To prevent the output from clearing immediately
    display(dropdown)
    plot_MC(change['new'])

# Combine
dropdown.observe(on_dropdown_change, names='value')

# Show plot and filter
display(dropdown)
plot_MC(dropdown.value)

"""# Medical Condition Breakdown #

### Age Group ###

Starting with the Age Group graph, we can see that there are stark differences in the diagnosis by groups. The 20-79 year olds make up the majority of the dataset, which accounts for the large gap. Additionally, some conditions (such as Arthritis) are seen primarily in older patients, and rarely seen in younger patients. When it comes to obesity, we can note that the patients aged 40-59 have the highest diagnosis rate.

### Gender ###

Looking at the Gender graph, there appears to be very little difference between male and female patients when it comes to diagnosis. The only slightly notable difference is in asthma, where there are 79 more male patients with the diagnosis than female patients.

### Blood Type ###
"""

# Correlation between Medical Condition and Billing Amount
plt.figure(figsize = (10, 6))
sns.barplot(x = 'Medical Condition', y = 'Billing Amount', data = df, palette = 'rainbow')  # maintaining consistency with prior graphs
plt.title('Billing Amount by Medical Condition')
plt.xlabel('Medical Condition')
plt.ylabel('Billing Amount')
plt.xticks(rotation = 45)
plt.show()

# Correlation between Test Results and Billing Amount
plt.figure(figsize = (10, 6))
sns.barplot(x = 'Test Results', y = 'Billing Amount', data = df, palette = 'rainbow')    # maintaining consistency with prior graphs
plt.title('Billing Amount by Test Results')
plt.xlabel('Test Results')
plt.ylabel('Billing Amount')
plt.xticks(rotation = 45)
plt.show()

"""## Correlations ##

As all of the values are close to 0, there does not appear to be any correlations between Age, Gender, Blood Type, or Medication and Medical Condition. Further analysis can give us a more accurate picture.

# Predicting Test Results Using Logistic Regression #

One way to improve patient care, and increase patient outcomes, is to determine if a patient is at risk of a condition before they begin showing signs of it. If we can predict whether a patient might have an abnormal test result, we can attempt to intervene early with education and additional diagnostic testing.
To do this, we will be looking primarily at medical conditions to examine the accuracy.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Check the values for 'Test Result'. Should be Normal, Abnormal, and Inconclusive
print("Unique values in 'Test Results':")
print(df['Test Results'].unique())

# Remove the 'Inconclusive'. It is not helpful and I'm making this binary to do LR
df = df[df['Test Results'] != 'Inconclusive']

print(df.head())

# Convert to binary variable. Abnormal will flag as 1, and normal will be 0
df['Test Results'] = df['Test Results'].apply(lambda x: 1 if x == 'Abnormal' else 0)

# Get columns
df = df[['Medical Condition', 'Test Results']]

# Encode Medical Condition to be used in regression
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), ['Medical Condition'])
    ],
    remainder = 'passthrough'    # Leave other columns alone
)

# Get features and targets
X = df[['Medical Condition']]
y = df['Test Results']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify = y)

# Create pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter = 1500))  # Iterate it a lot to get best fit
])

# Train the data
pipeline.fit(X_train, y_train)

# Get predictions
pred = pipeline.predict(X_test)

# Get accuracy and classification report
accuracy = accuracy_score(y_test, pred)
classification_report_result = classification_report(y_test, pred)

# Print Results
print("Accuracy: ", accuracy)
print("Classification Report: ")
print(classification_report_result)

"""## Analysis of the Model ##

Lookinng at the accuracy first, we can see that it has a score of 51%. This suggests the model is underperforming. This will be improved upon in future models.

For the Classification Report, the recall for normal results is at 35%, indicating that true positives are only given 35% of the time. In contrast, true positives for the abornmal results are given 67% of the time.

The results of this analysis suggest that it is not due to a class imbalance, so we can hypothesize that low pattern recognition is the issue.
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('healthcare_dataset.csv')

# Display the first few rows of the dataset
print(df.head())

# Convert date columns to datetime
df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

# Calculate length of stay
df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days

# Display the first few rows with the new feature
print(df[['Date of Admission', 'Discharge Date', 'Length of Stay']].head())

# Select features and target variable
X = df[['Age', 'Gender', 'Medical Condition', 'Test Results']]
y = df['Length of Stay']

# Display the first few rows of the selected features and target
print(X.head())
print(y.head())

# Load the dataset to check for outliers
df = pd.read_csv('healthcare_dataset.csv')

# Edit the date columns
df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

# Get length of hospital stay by taking discharge - admission date
df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days

# Get descriptive stats of the length (used to check RMSE below)
mean_length_of_stay = df['Length of Stay'].mean()
std_length_of_stay = df['Length of Stay'].std()

print('Mean Length of Stay: ', mean_length_of_stay)
print('Standard Deviation of Length of Stay: ', std_length_of_stay)

# Find outliers
q1 = df['Length of Stay'].quantile(0.25)
q3 = df['Length of Stay'].quantile(0.75)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr    # formula found online
upper_bound = q3 + 1.5 * iqr

# Create a list of outliers found
outliers = df[(df['Length of Stay'] < lower_bound) | (df['Length of Stay'] > upper_bound)]

# Find deviation
outliers['Deviation from Mean'] = outliers['Length of Stay'] - mean_length_of_stay

# Display the outliers with their deviation from the mean
outliers_list = outliers[['Length of Stay', 'Deviation from Mean']]
print(outliers_list)

print(sum(df["Admission Type"].isna()))

print(sum(df["Medical Condition"].isna()))

"""## Results of Model Analysis ##

A root mean square error of 8.36 shows that the model has potential to be off by 8.36 days. While this appears to be a concerning error, we can compare this to the standard deviation of the original data. The length of stay varied by 8.66 days. The root mean square error we received is close to the original standard deviation, indicating that the variation is simply a reflection of the natural varaibility of the data.

While the model has room for improvement, it currently performs adequately for the data used.
"""

# Import libraries again
import pandas as pd
import pickle

# Get the files we saved earlier
model = pickle.load(open('length_of_stay_model.pkl', 'rb'))
preprocessor = pickle.load(open('length_of_stay_preprocessor.pkl', 'rb'))

# Set up the predictions
def predict_length_of_stay(admission_type, medical_condition):   # Medical Condition and Admission Type as input variables
    input_data = pd.DataFrame([{"Admission Type": admission_type, "Medical Condition": medical_condition}])
    input_preprocessed = preprocessor.transform(input_data)       # Transform the data
    prediction = model.predict(input_preprocessed)       # Predict the length of stay
    return prediction[0]

# Get input variables from user
admission_type = input("Enter Admission Type (Emergency, Elective, Newborn, Trauma): ")
medical_condition = input("Enter Medical Condition: ")

# Predictions
predicted_length_of_stay = predict_length_of_stay(admission_type, medical_condition)

# Print results
print(f"Predicted Length of Stay: {predicted_length_of_stay:.2f} days")

"""# Cross-Validation"""

import pandas as pd
from sklearn.model_selection import LeaveOneOut, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from numpy import mean, absolute, sqrt

# Get data again to prevent potential accidental edits above
df = pd.read_csv('healthcare_dataset.csv')

# Edit the date into a datetime
df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

# Get the length of stay again
df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days

# Get sample of the data to limit runtime
df_sample = df.sample(1000, random_state=42)

# Get the target variables and features as before
X = df_sample[['Admission Type', 'Medical Condition']]
y = df_sample['Length of Stay']

# Label encoding the categorical variables (as discussed in meeting 2)
label_encoders = {}
for column in X.columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])
    label_encoders[column] = le

# Use LOOCV for valdiation
cv = LeaveOneOut()

# Use RFR
model = RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 42)

# Use LOOCV to evaluate the model
scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error', cv = cv, n_jobs = -1)

# Get RMSE
rmse = sqrt(mean(absolute(scores)))
print(f'Root Mean Squared Error (RMSE): {rmse}')

import pandas as pd
from numpy import mean, std, absolute, sqrt
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Get data again
df = pd.read_csv('healthcare_dataset.csv')

# Edit the date into a datetime
df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

# Get the length of stay
df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days

# Get a smaller sample of the data to prevent the kernel from crashing
df_sample = df.sample(500, random_state=42)

# Get the target variables and features
X = df_sample[['Admission Type', 'Medical Condition']]
y = df_sample['Length of Stay']

# Label encoding the categorical variables (as discussed in meeting 2)
label_encoders = {}
for column in X.columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column].copy())
    label_encoders[column] = le

# Use K-Fold this time
cv = KFold(n_splits=10, random_state=1, shuffle=True)

# Use RFR
model = RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 42)

# Get the CV scores
scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error', cv = cv, n_jobs = -1)

# Get RMSE and STD
rmse_scores = sqrt(absolute(scores))
print('RMSE: %.3f (%.3f)' % (mean(rmse_scores), std(rmse_scores)))

import pandas as pd
from numpy import mean, std, absolute, sqrt
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor

# Load data
df = pd.read_csv('healthcare_dataset.csv')

# Edit the date into a datetime
df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

# Calculate length of stay
df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days

# Get a smaller sample of the data to prevent the kernel from crashing
df_sample = df.sample(1000, random_state=42)

# Get the target variables and features. Add 'Age', 'Gender', and 'Hospital' to the feature set
X = df_sample[['Admission Type', 'Medical Condition', 'Age', 'Gender', 'Doctor', 'Insurance Provider', 'Hospital']]
y = df_sample['Length of Stay']

# Label encoding the categorical variables
label_encoders = {}
for column in ['Admission Type', 'Medical Condition', 'Gender', 'Doctor', 'Insurance Provider', 'Hospital']:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column].copy())
    label_encoders[column] = le

# Define cross-validation method
cv = KFold(n_splits=10, random_state=1, shuffle=True)

# Build the model
model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)

# Use k-fold cross-validation to evaluate the model
scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)

# Calculate and print RMSE
rmse_scores = sqrt(absolute(scores))
print('RMSE: %.3f (%.3f)' % (mean(rmse_scores), std(rmse_scores)))

"""# Results #

### LeaveOneOut ###
After running this validation, we are left with a root mean square error of 8.62. This is similar to the previous RMSE and the standard deviation of length of stay (around 8 days). This error decreases from 10.12 days to 8.62 days when increasing the sample from 100 to 1000.


### K-Fold ###
After using the K-Fold method, we are left with a RMSE of 8.74. The standard deviation between folds was 0.691, which is relatively low. This indicates it performs consistently well across each fold.

Adding in additional features, the RMSE does increase to 8.77, however, the STD lowers to 0.347.

Overall, the model still has the lowest deviation using the RandomForestRegressor model, and is consistently showing an error of approximately 8 days.
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold, cross_val_predict
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor

# Load data
df = pd.read_csv('healthcare_dataset.csv')

# Edit the date into a datetime
df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

# Calculate length of stay
df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days

# With the original 2 columns to test against added features
X = df[['Admission Type', 'Medical Condition']]
y = df['Length of Stay']

# Label encoding the categorical variables
label_encoders = {}
for column in X.columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column].copy())
    label_encoders[column] = le

# K-Fold
cv = KFold(n_splits=10, random_state=1, shuffle=True)

# RFR
model = RandomForestRegressor(n_estimators=400, max_depth=20, random_state=42)

# Predictions with CV
predictions = cross_val_predict(model, X, y, cv = cv)

# Get residuals
residuals = y - predictions

# Plot as requested in meeting 2
plt.scatter(predictions, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Predicted Values vs Residuals (Initial Model)')
plt.show()

# Do again with added features
X = df[['Admission Type', 'Medical Condition', 'Age', 'Gender', 'Doctor', 'Insurance Provider', 'Hospital']]

# LE
label_encoders = {}
for column in ['Admission Type', 'Medical Condition', 'Gender', 'Doctor', 'Insurance Provider', 'Hospital']:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column].copy())
    label_encoders[column] = le

# KFCV
scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)

# RMSE
rmse_scores = sqrt(absolute(scores))
print('RMSE: %.3f (%.3f)' % (mean(rmse_scores), std(rmse_scores)))

# Get predictions
predictions = cross_val_predict(model, X, y, cv = cv)

# Get residuals
residuals = y - predictions

# Plot as requested in meeting 2
plt.scatter(predictions, residuals)
plt.axhline(y = 0, color = 'r', linestyle = '--')     # Add line
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Predicted Values vs Residuals (Updated Model with Additional Features)')
plt.show()

model.feature_importances_

import pickle

# Save to file
with open('length_of_stay_model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)
with open('length_of_stay_encoders.pkl', 'wb') as encoders_file:
    pickle.dump(label_encoders, encoders_file)

import pandas as pd
import pickle
import numpy as np

# Get data again
df = pd.read_csv('healthcare_dataset.csv')

# Get all possible values in the dataset
admission_types = df['Admission Type'].unique()
medical_conditions = df['Medical Condition'].unique()
genders = df['Gender'].unique()
doctors = df['Doctor'].unique()
insurance_providers = df['Insurance Provider'].unique()
hospitals = df['Hospital'].unique()

# Load saved files
model = pickle.load(open('length_of_stay_model.pkl', 'rb'))
label_encoders = pickle.load(open('length_of_stay_encoders.pkl', 'rb'))

# Get prediction functions (now with added features)
def predict_length_of_stay(admission_type, medical_condition, age, gender, doctor, insurance_provider, hospital):

    # Create a dataframe
    input_data = pd.DataFrame([{
        "Admission Type": admission_type,
        "Medical Condition": medical_condition,
        "Age": age,
        "Gender": gender,
        "Doctor": doctor,
        "Insurance Provider": insurance_provider,
        "Hospital": hospital
    }])

    # This handles unknown errors. The model does not recognize some values, so this was added to address that.
    for column in ['Admission Type', 'Medical Condition', 'Gender', 'Doctor', 'Insurance Provider', 'Hospital']:
        le = label_encoders[column]
        input_data[column] = input_data[column].map(lambda s: '<unknown>' if s not in le.classes_ else s)   # If the value is not already stored, store it in the "unknown" string
        le.classes_ = np.append(le.classes_, '<unknown>')    # If there is something in the unknown classes, add it to the encoder
        input_data[column] = le.transform(input_data[column])

    # Predict LOS
    prediction = model.predict(input_data)
    return prediction[0]

# Show user the options (it is not the prettiest. I'll address that later.)
def get_user_input(options, prompt):
    print(f"Select {prompt} from the following options:")
    for i, option in enumerate(options):
        print(f"{i}: {option}")
    selected_index = int(input(f"Enter the number corresponding to the {prompt}: "))
    return options[selected_index]

# Get inputs
admission_type = get_user_input(admission_types, "Admission Type")
medical_condition = get_user_input(medical_conditions, "Medical Condition")
age = int(input("Enter Age: "))
gender = get_user_input(genders, "Gender")
doctor = get_user_input(doctors, "Doctor")
insurance_provider = get_user_input(insurance_providers, "Insurance Provider")
hospital = get_user_input(hospitals, "Hospital")

# Predict LOS
predicted_length_of_stay = predict_length_of_stay(admission_type, medical_condition, age, gender, doctor, insurance_provider, hospital)

# Show LOS
print(f"Predicted Length of Stay: {predicted_length_of_stay:.2f} days")

"""## NOTES: ##

I cannot get streamlit to work on my terminal. I have tried updating packages, asking chatGPT, etc. I will try this weekend to redo it and use a different terminal to determine if mine is the problem.
"""

import streamlit as st
import pandas as pd
import pickle

# Load data
df = pd.read_csv('healthcare_dataset.csv')

# Get values
admission_types = df['Admission Type'].unique()
medical_conditions = df['Medical Condition'].unique()
genders = df['Gender'].unique()
doctors = df['Doctor'].unique()
insurance_providers = df['Insurance Provider'].unique()
hospitals = df['Hospital'].unique()

# Load saved files
model = pickle.load(open('length_of_stay_model.pkl', 'rb'))
label_encoders = pickle.load(open('length_of_stay_encoders.pkl', 'rb'))

# Get pred function
def predict_length_of_stay(admission_type, medical_condition, age, gender, doctor, insurance_provider, hospital):
    # Create a dataframe for the input
    input_data = pd.DataFrame([{
        "Admission Type": admission_type,
        "Medical Condition": medical_condition,
        "Age": age,
        "Gender": gender,
        "Doctor": doctor,
        "Insurance Provider": insurance_provider,
        "Hospital": hospital
    }])

    # LE
    for column in ['Admission Type', 'Medical Condition', 'Gender', 'Doctor', 'Insurance Provider', 'Hospital']:
        input_data[column] = label_encoders[column].transform(input_data[column])

    # Predict LOS
    prediction = model.predict(input_data)
    return prediction[0]

# Set up streamlit title
st.title('Healthcare Length of Stay Prediction')

# Add select boxes for better user experience
admission_type = st.selectbox('Select Admission Type', admission_types)
medical_condition = st.selectbox('Select Medical Condition', medical_conditions)
age = st.number_input('Enter Age', min_value=0, max_value=120, value=30)
gender = st.selectbox('Select Gender', genders)
doctor = st.selectbox('Select Doctor', doctors)
insurance_provider = st.selectbox('Select Insurance Provider', insurance_providers)
hospital = st.selectbox('Select Hospital', hospitals)

if st.button('Predict Length of Stay'):
    predicted_length_of_stay = predict_length_of_stay(admission_type, medical_condition, age, gender, doctor, insurance_provider, hospital)
    st.write(f'Predicted Length of Stay: {predicted_length_of_stay:.2f} days')